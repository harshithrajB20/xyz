{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfJ5pA2CBEjYKDdI4EDuI/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshithrajB20/xyz/blob/main/nlp_lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBteFq8-Z5tR",
        "outputId": "cf0dbe7e-b913-49ff-9302-aaa09fb5bce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5jtUiq-T3syE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulgTCHb3a0bC",
        "outputId": "21996995-7a95-4a48-84e7-4237a728c0e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "hindi_text='इसे आज़माने के लिए'\n",
        "tokens=word_tokenize(hindi_text)\n",
        "print(\"Tokenize words(NLTK):\",tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuy0hrYwbkaP",
        "outputId": "2c69c35d-cff2-475b-8bf3-555c7f86a896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize words(NLTK): ['इसे', 'आज़माने', 'के', 'लिए']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "hindi_text='इसे आज़माने के लिए'\n",
        "sentences=sent_tokenize(hindi_text)\n",
        "print(\"Segmented sentences(NLTK):\",sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGixglZ-eBc2",
        "outputId": "749517d5-14f7-4528-a16a-0b517ad2de0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmented sentences(NLTK): ['इसे आज़माने के लिए']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install indic-nlp-library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLq0Lpc3eqiX",
        "outputId": "f10f4aab-56ac-4dac-e51c-95cf73200abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.2)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (8.1.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.16.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.2)\n",
            "Requirement already satisfied: tomli>=2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2024.12.14)\n",
            "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: morfessor, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.tokenize import sentence_tokenize,indic_tokenize\n",
        "from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n",
        "hindi_text='इसे आज़माने के लिए'\n",
        "sentences=sentence_tokenize.sentence_split(hindi_text,lang='hi')\n",
        "print(\"Segmented sentences(IndicNLP):\",sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsF0Y0mhfRfK",
        "outputId": "b4cce19b-8676-4dc5-f1ac-6f87139bd1aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmented sentences(IndicNLP): ['इसे आज़माने के लिए']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pat=re.compile('[.?!|]')\n",
        "tokens=pat.split(hindi_text)\n",
        "print(\"Tokenize words(Regex):\",tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb49-TsakWXx",
        "outputId": "48ffa4a7-d61d-4e6d-b8ce-57b54ffaf3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize words(Regex): ['इसे आज़माने के लिए']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "words = ['cared','university','fairly','easily','singing',\n",
        "'sings','sung','singer','sportingly','magnified']\n",
        "st = LancasterStemmer()\n",
        "\n",
        "print(\"Lanchaster Stemmer\")\n",
        "\n",
        "stem_words = []\n",
        "for w in words:\n",
        "    x = st.stem(w)\n",
        "    stem_words.append(x)\n",
        "\n",
        "\n",
        "for e1,e2 in zip(words,stem_words):\n",
        "    print(e1+' ----> '+e2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkAvMiuq3t7o",
        "outputId": "1a22a0fb-bd29-454b-f684-40260d813aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lanchaster Stemmer\n",
            "cared ----> car\n",
            "university ----> univers\n",
            "fairly ----> fair\n",
            "easily ----> easy\n",
            "singing ----> sing\n",
            "sings ----> sing\n",
            "sung ----> sung\n",
            "singer ----> sing\n",
            "sportingly ----> sport\n",
            "magnified ----> magn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "class NgramModel:\n",
        "    def __init__(self, n, k=1):\n",
        "        self.n = n\n",
        "        self.k = k\n",
        "        self.ngram_counts = defaultdict(int)\n",
        "        self.context_counts = defaultdict(int)\n",
        "\n",
        "    def train(self, corpus):\n",
        "        for sentence in corpus:\n",
        "            words = sentence.split()\n",
        "            for i in range(len(words) - self.n + 1):\n",
        "                ngram = tuple(words[i:i+self.n-1])\n",
        "                next_word = words[i+self.n-1]\n",
        "                self.ngram_counts[(ngram, next_word)] += 1\n",
        "                self.context_counts[ngram] += 1\n",
        "\n",
        "    def probability(self, ngram, next_word):\n",
        "        context = ngram\n",
        "        context_count = self.context_counts[context]\n",
        "        ngram_count = self.ngram_counts[(ngram, next_word)]\n",
        "        unique_words = len(self.ngram_counts)\n",
        "        return (ngram_count + self.k) / (context_count + self.k * unique_words)\n",
        "\n",
        "    def generate_next_word(self, ngram):\n",
        "        possible_next_words = [word for ngram_, word in self.ngram_counts.keys() if ngram_ == ngram]\n",
        "        probabilities = [self.probability(ngram, word) for word in possible_next_words]\n",
        "        probabilities = np.array(probabilities)\n",
        "        probabilities /= probabilities.sum()\n",
        "        return np.random.choice(possible_next_words, p=probabilities)\n",
        "\n",
        "\n",
        "corpus = [\"this is a cat\", \"this is a dog\", \"this is a cat\"]\n",
        "ngram_model = NgramModel(n=2, k=1)\n",
        "ngram_model.train(corpus)\n",
        "ngram = (\"is\",)\n",
        "next_word = ngram_model.generate_next_word(ngram)\n",
        "print(next_word)\n"
      ],
      "metadata": {
        "id": "8aLQZ5wO3zc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a470b4-9686-4958-fd75-ff7cb7090038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "class NgramModel:\n",
        "    def __init__(self, n, k=1):\n",
        "        self.n = n\n",
        "        self.k = k\n",
        "        self.ngram_counts = defaultdict(int)\n",
        "        self.context_counts = defaultdict(int)\n",
        "\n",
        "    def train(self, corpus):\n",
        "        for sentence in corpus:\n",
        "            words = sentence.split()\n",
        "            for i in range(len(words) - self.n + 1):\n",
        "                ngram = tuple(words[i:i+self.n-1])\n",
        "                next_word = words[i+self.n-1]\n",
        "                self.ngram_counts[(ngram, next_word)] += 1\n",
        "                self.context_counts[ngram] += 1\n",
        "\n",
        "    def probability(self, ngram, next_word):\n",
        "        context = ngram\n",
        "        context_count = self.context_counts[context]\n",
        "        ngram_count = self.ngram_counts[(ngram, next_word)]\n",
        "        unique_words = len(self.ngram_counts)\n",
        "\n",
        "        return (ngram_count + self.k) / (context_count + self.k * unique_words)\n",
        "\n",
        "    def generate_next_word(self, ngram):\n",
        "        possible_next_words = [word for ngram_, word in self.ngram_counts.keys() if ngram_ == ngram]\n",
        "        probabilities = [self.probability(ngram, word) for word in possible_next_words]\n",
        "        probabilities = np.array(probabilities)\n",
        "        probabilities /= probabilities.sum()\n",
        "        return np.random.choice(possible_next_words, p=probabilities)\n",
        "\n",
        "    def ngram_frequency_distribution(self):\n",
        "        return dict(self.ngram_counts)\n",
        "\n",
        "    def ngram_probabilities(self):\n",
        "        probabilities = {}\n",
        "        for (ngram, next_word), count in self.ngram_counts.items():\n",
        "            probabilities[(ngram, next_word)] = self.probability(ngram, next_word)\n",
        "        return probabilities\n",
        "\n",
        "\n",
        "corpus = [\"this is a cat\", \"this is a dog\", \"this is a cat\"]\n",
        "ngram_model = NgramModel(n=2, k=1)\n",
        "ngram_model.train(corpus)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ngram_freq_dist = ngram_model.ngram_frequency_distribution()\n",
        "print(\"N-gram Frequency Distribution:\", ngram_freq_dist)\n",
        "\n",
        "\n",
        "ngram_probs = ngram_model.ngram_probabilities()\n",
        "print(\"N-gram Probabilities:\", ngram_probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AOYg3k5DZV6",
        "outputId": "9acf9175-59a3-4321-88e1-e1290dc0c5cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N-gram Frequency Distribution: {(('this',), 'is'): 3, (('is',), 'a'): 3, (('a',), 'cat'): 2, (('a',), 'dog'): 1}\n",
            "N-gram Probabilities: {(('this',), 'is'): 0.5714285714285714, (('is',), 'a'): 0.5714285714285714, (('a',), 'cat'): 0.42857142857142855, (('a',), 'dog'): 0.2857142857142857}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "WdcE_mvl7QRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv57kffI7UXK",
        "outputId": "be785db5-b873-4a5a-a153-95c653722d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y514LkJi7euB",
        "outputId": "64cb67db-46e1-457d-87e3-6ef0d86091e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "# Input sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "# Perform POS tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "# Print the tagged words\n",
        "for word, tag in pos_tags:\n",
        " print(f\"Word: {word}, POS Tag: {tag}\")"
      ],
      "metadata": {
        "id": "IHSemJXwJzS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ad92bf-3f5a-4302-8436-6155034989bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: The, POS Tag: DT\n",
            "Word: quick, POS Tag: JJ\n",
            "Word: brown, POS Tag: NN\n",
            "Word: fox, POS Tag: NN\n",
            "Word: jumps, POS Tag: VBZ\n",
            "Word: over, POS Tag: IN\n",
            "Word: the, POS Tag: DT\n",
            "Word: lazy, POS Tag: JJ\n",
            "Word: dog, POS Tag: NN\n",
            "Word: ., POS Tag: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "verbs = [word for word, pos in tagged if pos.startswith('VB')]\n",
        "\n",
        "print(\"Nouns:\", nouns)\n",
        "print(\"Verbs:\", verbs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQnM-7bb7Paj",
        "outputId": "c6e3782f-0688-4309-fe6f-804d6ba2304f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nouns: ['brown', 'fox', 'dog']\n",
            "Verbs: ['jumps']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "chunking_grammar = r\"\"\"\n",
        " NP: {<DT|JJ|NN.*>+}\n",
        " VP: {<VB.*><NP|PP>*}\n",
        "\"\"\"\n",
        "\n",
        "chunk_parser = RegexpParser(chunking_grammar)\n",
        "\n",
        "chunks = chunk_parser.parse(pos_tags)\n",
        "\n",
        "chunks.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF0A47ns7GtD",
        "outputId": "39858d51-a09b-4259-90dd-98e4a5fccabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                               S                                           \n",
            "    ___________________________|________________________________            \n",
            "   |     |            NP                        VP              NP         \n",
            "   |     |     _______|________________         |        _______|______     \n",
            "over/IN ./. The/DT quick/JJ brown/NN fox/NN jumps/VBZ the/DT lazy/JJ dog/NN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Sample text\n",
        "text = \"Your sample text goes here.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Tag the tokens\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "print(tagged)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCDkzTtIAFYx",
        "outputId": "c21a5673-d6a4-4f05-f80d-641e77d51610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Your', 'PRP$'), ('sample', 'NN'), ('text', 'NN'), ('goes', 'VBZ'), ('here', 'RB'), ('.', '.')]\n"
          ]
        }
      ]
    }
  ]
}